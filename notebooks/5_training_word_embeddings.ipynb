{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1332dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e819693",
   "metadata": {},
   "source": [
    "## Data wikitext-2\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good articles on Wikipedia.\n",
    "\n",
    "The data can be dowloaded using: `wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e38bc332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \r\n"
     ]
    }
   ],
   "source": [
    "! head -5  wikitext-2/wiki.train.tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77bc9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37a384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ba75f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = read_file(\"wikitext-2/wiki.train.tokens\")\n",
    "valid_tokens = read_file(\"wikitext-2/wiki.valid.tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f82dad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_vocab(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64b79cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b244de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_from_word_count(word_count):\n",
    "    for word in list(word_count):\n",
    "        if word_count[word] < 5:\n",
    "            del word_count[word]\n",
    "        \n",
    "    vocab2index = {\"UNK\": 0}\n",
    "    words = [\"UNK\"]\n",
    "    for word in word_count:\n",
    "        vocab2index[word] = len(words)\n",
    "        words.append(word)\n",
    "    return vocab2index, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a78d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index, words = get_vocab_from_word_count(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f20e67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21590"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef96a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'Chronicles',\n",
       " '=',\n",
       " 'III',\n",
       " 'Valkyria',\n",
       " 'runs',\n",
       " 'video',\n",
       " 'pitted',\n",
       " 'and',\n",
       " 'nation']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4021eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, text, vocab2index, K=3):\n",
    "        tokens = np.concatenate([x.split() for x in text])\n",
    "        self.text = np.array([ vocab2index.get(x, 0) for x in tokens])\n",
    "        self.K = K\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.K\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx:idx+self.K], self.text[idx+self.K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b2fb1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fee20fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.concatenate([x.split() for x in train_tokens[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0ec8d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['=', 'Valkyria', 'Chronicles', 'III', '='], dtype='<U32')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ea93567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1, 3, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ vocab2index.get(x, 0) for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8fc9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_train_ds = LanguageModelDataset(train_tokens[:3], vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "90781a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 4, 1]), 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94c7118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 1, 3]), 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2640e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toy_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4ce9ee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aaf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restricting text for faster training\n",
    "train_ds = LanguageModelDataset(train_tokens[:5000], vocab2index)\n",
    "valid_ds = LanguageModelDataset(valid_tokens[:100], vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba43c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=50, K=3, M=100):\n",
    "        \"\"\"Initialize an embedding layer and a linear layer\n",
    "        \"\"\"\n",
    "        super(NeuralModel, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear1 = nn.Linear(K*emb_size, M)\n",
    "        self.linear2 = nn.Linear(M, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef4e516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=1000, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "017e56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d5d7709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "model = NeuralModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6515840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 21590])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "663a0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in valid_dl:\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b1faf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(losses)\n",
    "        val_loss = val_metrics(model, valid_dl)\n",
    "        \n",
    "        print(\"train_loss %.3f train_ppl %.3f val_loss %.3f val_ppl %.3f\" % (\n",
    "            train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "376e3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.993993918100992"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e4a0edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02787bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "model = NeuralModel(vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16a968bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 60.789 train_ppl 251339632163322557108322304.000 val_loss 104.546 val_ppl 2533378217971242035698838669910209404961030144.000\n",
      "train_loss 34.861 train_ppl 1380432020013199.000 val_loss 35.286 val_ppl 2111226875019214.750\n",
      "train_loss 12.375 train_ppl 236814.552 val_loss 49.514 val_ppl 3189923016260578705408.000\n",
      "train_loss 7.548 train_ppl 1896.828 val_loss 50.759 val_ppl 11079817131936681295872.000\n",
      "train_loss 7.318 train_ppl 1506.507 val_loss 50.863 val_ppl 12293597738840757370880.000\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "356ed5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 7.101 train_ppl 1212.768 val_loss 50.509 val_ppl 8625494110465340473344.000\n",
      "train_loss 6.871 train_ppl 964.270 val_loss 50.415 val_ppl 7849021847812407558144.000\n",
      "train_loss 6.829 train_ppl 924.011 val_loss 50.376 val_ppl 7550684868131036332032.000\n",
      "train_loss 6.815 train_ppl 911.303 val_loss 50.350 val_ppl 7358852457708025544704.000\n",
      "train_loss 6.810 train_ppl 906.539 val_loss 50.367 val_ppl 7487324502420585709568.000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "train_epocs(model, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "93aaa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 6.809 train_ppl 905.969 val_loss 50.359 val_ppl 7423465666402831040512.000\n",
      "train_loss 6.808 train_ppl 905.397 val_loss 50.359 val_ppl 7421399309995187634176.000\n",
      "train_loss 6.809 train_ppl 906.262 val_loss 50.366 val_ppl 7476112253852637462528.000\n",
      "train_loss 6.809 train_ppl 906.062 val_loss 50.374 val_ppl 7533881104917638152192.000\n",
      "train_loss 6.809 train_ppl 906.279 val_loss 50.370 val_ppl 7502425736906208706560.000\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f3a5f2",
   "metadata": {},
   "source": [
    "## Lab:\n",
    "Write a pipeline for word2vec with negative sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995c81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
