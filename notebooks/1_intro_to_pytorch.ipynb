{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch consists of 4 main packages:\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU\n",
    "* torch.autograd: a package for automatically obtaining gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "Like Numpy tensors but can utilize GPUs to accelerate its numerical computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensors from lists or numpy arrays\n",
    "x = torch.tensor([[1, 2],[3, 4]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensor\n",
    "N = 5\n",
    "x = torch.randn(N, 10).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5202e-01,  1.5325e+00,  2.3238e-01,  1.3697e-01, -1.8312e-01,\n",
       "         -4.0452e-01,  3.1694e-01, -4.0636e-01, -1.2905e+00,  5.8617e-01],\n",
       "        [-1.2913e-01, -9.8696e-02, -4.2240e-01, -7.2865e-01, -2.4308e-01,\n",
       "          1.2958e+00, -6.1219e-01,  8.1288e-01, -1.5446e-01,  1.6568e+00],\n",
       "        [-3.0231e-01,  1.3835e-01, -1.0783e+00,  1.3272e+00, -8.1760e-01,\n",
       "         -1.8135e-01, -4.2698e-01, -3.0336e-01,  4.7895e-01,  5.2039e-01],\n",
       "        [-2.3563e+00, -2.2943e+00, -4.0159e-01,  9.4906e-01,  1.4295e-02,\n",
       "         -2.7297e+00, -1.3179e+00,  5.8888e-01, -1.2266e+00, -5.1505e-02],\n",
       "        [ 2.8930e-01, -5.3817e-01,  2.0013e-01, -1.7574e-03, -1.3387e+00,\n",
       "          6.4940e-01,  8.2567e-01, -6.8555e-01,  2.1885e+00, -1.1110e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5202e-01,  1.5325e+00,  2.3238e-01,  1.3697e-01, -1.8312e-01,\n",
       "         -4.0452e-01,  3.1694e-01, -4.0636e-01, -1.2905e+00,  5.8617e-01,\n",
       "         -1.2913e-01, -9.8696e-02, -4.2240e-01, -7.2865e-01, -2.4308e-01,\n",
       "          1.2958e+00, -6.1219e-01,  8.1288e-01, -1.5446e-01,  1.6568e+00,\n",
       "         -3.0231e-01,  1.3835e-01, -1.0783e+00,  1.3272e+00, -8.1760e-01,\n",
       "         -1.8135e-01, -4.2698e-01, -3.0336e-01,  4.7895e-01,  5.2039e-01,\n",
       "         -2.3563e+00, -2.2943e+00, -4.0159e-01,  9.4906e-01,  1.4295e-02,\n",
       "         -2.7297e+00, -1.3179e+00,  5.8888e-01, -1.2266e+00, -5.1505e-02,\n",
       "          2.8930e-01, -5.3817e-01,  2.0013e-01, -1.7574e-03, -1.3387e+00,\n",
       "          6.4940e-01,  8.2567e-01, -6.8555e-01,  2.1885e+00, -1.1110e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping of tensors using .view()\n",
    "x.view(1,-1) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.5202258e-01,  1.5324641e+00,  2.3238428e-01,  1.3697372e-01,\n",
       "        -1.8312012e-01, -4.0451506e-01,  3.1694263e-01, -4.0635753e-01,\n",
       "        -1.2905294e+00,  5.8617008e-01],\n",
       "       [-1.2913226e-01, -9.8695815e-02, -4.2239875e-01, -7.2865218e-01,\n",
       "        -2.4308336e-01,  1.2957559e+00, -6.1218745e-01,  8.1288332e-01,\n",
       "        -1.5445977e-01,  1.6567501e+00],\n",
       "       [-3.0230689e-01,  1.3834929e-01, -1.0783136e+00,  1.3271898e+00,\n",
       "        -8.1759948e-01, -1.8134765e-01, -4.2697915e-01, -3.0335742e-01,\n",
       "         4.7895098e-01,  5.2039194e-01],\n",
       "       [-2.3563495e+00, -2.2943420e+00, -4.0158853e-01,  9.4906092e-01,\n",
       "         1.4295382e-02, -2.7297289e+00, -1.3179215e+00,  5.8887899e-01,\n",
       "        -1.2265542e+00, -5.1504571e-02],\n",
       "       [ 2.8930184e-01, -5.3816849e-01,  2.0013089e-01, -1.7574476e-03,\n",
       "        -1.3387172e+00,  6.4939690e-01,  8.2567465e-01, -6.8555075e-01,\n",
       "         2.1885130e+00, -1.1110328e+00]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensors to numpy arrays\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "The autograd package in PyTorch provides classes and functions implementing automatic differentiation of arbitrary scalar valued function. For example, the gradient of the error with respect to all parameters.\n",
    "\n",
    "`requires_grad=True` tells PyTorch that it needs to calculate the gradient with respect to this tensor.Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5., 6.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equivalent\n",
    "x = torch.tensor([1., 2., 3., 4., 5., 6.]).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  9., 19., 33., 51., 73.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*x**2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(188., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = (2*x**2 +1).sum()\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward() # computes the grad of L with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12., 16., 20., 24.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6116, -0.0059,  0.6791],\n",
       "        [ 1.0457, -2.6952,  0.2817]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is another example\n",
    "x = torch.randn(2, 3)\n",
    "x.requires_grad = True\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4957, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = (x**2).sum()\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2233, -0.0118,  1.3581],\n",
       "        [ 2.0914, -5.3905,  0.5633]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.backward()\n",
    "x.grad # note, it is the same shape as x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detach()\n",
    "The detach() method constructs a new view on a tensor which is declared not to need gradients. This may be needed for example when you want to take the output to a model to numpy to compute a metric with sklean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `x.numpy()` on x after the previous computation. See what happens. How would you fix this error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with torch.no_grad()\n",
    "Prevent the gradients from being calculated in a piece of code. This is useful at validation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "A neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear(5, 3)` creates a linear transformation with parameters $A$ and $b$ ($A\\cdot X+b$). Given an input matrix of observvation $X$ ($N \\times 5$), `nn.Linear(5, 3)` transforms X into a $N \\times 3$ matrix, where $N$ can be anything (number of observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3280,  0.4061,  0.2603, -0.0183, -0.4411],\n",
       "         [ 0.2967,  0.0547,  0.1132,  0.1029, -0.0365],\n",
       "         [-0.0496, -0.0309,  0.4068, -0.2464, -0.4372]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0487,  0.2481, -0.4347], requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters are initialized randomly\n",
    "list(linear_map.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 5]), torch.Size([3])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of parameters\n",
    "[p.shape for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of elements per parameter tensor. \n",
    "[p.numel() for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a layer with 20 input features  and 10 output features. Compute how many total parameters do you have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to fit a line to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x, y\n",
    "\n",
    "x, y = gen_fake_data(50, 3., 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to find **parameters** (weights) $a$ and $b$ such that you minimize the *error* between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error** ($\\sum_i (\\hat{y}_i - y_i)^2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y): return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.530150475199336"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lin(10,5,x)\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a, b, x, y): return mse(lin(a,b,x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.530150475199336"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(10, 5, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for $a$ and $b$? How do we find the best *fitting* linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed dataset $x$ and $y$ `mse_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.\n",
    "\n",
    "**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n",
    "\n",
    "Here is gradient descent implemented in [PyTorch](http://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate some more data\n",
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap x and y as tensor \n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2641], dtype=torch.float64, requires_grad=True),\n",
       " tensor([1.3254], dtype=torch.float64, requires_grad=True))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random Tensors for weights, and wrap them in tensors.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "a, b = np.random.randn(1), np.random.randn(1)\n",
    "a = torch.tensor(a, requires_grad=True)\n",
    "b = torch.tensor(b, requires_grad=True)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.8644056414023\n",
      "0.5311145624067163\n",
      "0.09443281199220184\n",
      "0.0911740379017952\n",
      "0.09076106284568813\n",
      "0.09045797784254733\n",
      "0.0902260152974644\n",
      "0.09004842299277167\n",
      "0.08991245656822082\n",
      "0.08980835932664152\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    loss = mse_loss(a,b,x,y)\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "    \n",
    "    # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call a.grad and b.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to a and b respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update a and b using gradient descent; a.data and b.data are Tensors,\n",
    "    # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    \n",
    "    # Zero the gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0766], dtype=torch.float64, requires_grad=True) tensor([7.9584], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# not that a and b should be close to 3 and 8 respectively\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified GD Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1, out_features=1, bias=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear tranformation with input dimension=1 and output dimension=1\n",
    "nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple way of specifying a linear regression model\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(1, 1),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent way of specifiying the same model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        return x \n",
    "model =  LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8858]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8959], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# note here we have just two parameters, why?\n",
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you have to be careful with the dimensions that your model is expecting\n",
    "# unsqueeze dim=1 transforms [10000] to [10000, 1]\n",
    "x = torch.unsqueeze(x, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0192],\n",
      "        [1.4533],\n",
      "        [1.6491],\n",
      "        ...,\n",
      "        [1.6278],\n",
      "        [1.2806],\n",
      "        [1.1278]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(x)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(67.0558, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.unsqueeze(1)\n",
    "F.mse_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "x_val, y_val = gen_fake_data(1000, 3., 8.)\n",
    "x_val = torch.tensor(x_val).float().unsqueeze(1)\n",
    "y_val = torch.tensor(y_val).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Use the optim package to define an Optimizer that will update the weights of the model for us. Here we will use Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 67.056 valid loss 64.666\n",
      "train loss 0.101 valid loss 0.102\n",
      "train loss 0.093 valid loss 0.095\n",
      "train loss 0.091 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n",
      "train loss 0.090 valid loss 0.094\n"
     ]
    }
   ],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    model.train() # some layers have different behavior during train/and evaluation\n",
    "    y_hat = model(x)\n",
    "    loss = F.mse_loss(y_hat, y)\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # computes gradients\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # checking validation loss\n",
    "    model.eval()  # some layers have different behavior during train/and evaluation\n",
    "    y_hat_val = model(x_val)\n",
    "    val_loss = F.mse_loss(y_hat_val, y_val)\n",
    "    \n",
    "    if t % 1000 == 0: print(\"train loss %.3f valid loss %.3f\" % (loss.item(), val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.0010]], requires_grad=True), Parameter containing:\n",
      "tensor([7.9986], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating fake data\n",
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_logistic_fake_data(n, a, b):\n",
    "    x = np.random.uniform(-20,20, (n, 2))\n",
    "    x2_hat = lin(a,b, x[:,0])\n",
    "    y = x[:,1] > x2_hat\n",
    "    return x, y.astype(int)\n",
    "\n",
    "x, y = gen_logistic_fake_data(100, 1., 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ae8c1ce80>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(-20, 20, 0.2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[:,0],x[:,1],c=y, s=8)\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\");\n",
    "plt.plot(t, t + 0.5, '--', c='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating train and val data\n",
    "x, y = gen_logistic_fake_data(10000, 1., 0.5)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float().unsqueeze(1)\n",
    "\n",
    "x_val, y_val = gen_logistic_fake_data(1000, 1., 0.5)\n",
    "x_val = torch.tensor(x_val).float()\n",
    "y_val = torch.tensor(y_val).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.245 valid loss 0.751\n",
      "train loss 0.018 valid loss 0.017\n",
      "train loss 0.014 valid loss 0.013\n",
      "train loss 0.012 valid loss 0.011\n",
      "train loss 0.010 valid loss 0.009\n",
      "train loss 0.008 valid loss 0.008\n",
      "train loss 0.007 valid loss 0.006\n",
      "train loss 0.006 valid loss 0.005\n",
      "train loss 0.005 valid loss 0.005\n",
      "train loss 0.004 valid loss 0.004\n"
     ]
    }
   ],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    model.train()\n",
    "    y_hat = model(x)\n",
    "    loss = F.binary_cross_entropy(torch.sigmoid(y_hat), y)\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat_val = model(x_val)\n",
    "    val_loss = F.binary_cross_entropy(torch.sigmoid(y_hat_val), y_val)\n",
    "    \n",
    "    if t % 1000 == 0: print(\"train loss %.3f valid loss %.3f\" % (loss.item(), val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-10.6495,  10.6255]], requires_grad=True), Parameter containing:\n",
      "tensor([-5.4558], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Instead of using `F.binary_cross_entropy(torch.sigmoid(y_hat), y)` try `F.binary_cross_entropy_with_logits(y_hat, y)`. Look at the documentation for `F.binary_cross_entropy_with_logits`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to take a vector back to numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_logistic_fake_data(10, 1., 0.5)\n",
    "x = torch.tensor(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.552051 ,   5.042107 ],\n",
       "       [-10.704544 ,   8.161397 ],\n",
       "       [-15.575344 ,  -7.0837326],\n",
       "       [  9.569121 ,  -1.9044411],\n",
       "       [  6.4253044,   7.5156593],\n",
       "       [ -4.6348243,  10.37898  ],\n",
       "       [ 14.983649 , -17.890064 ],\n",
       "       [-10.62132  , -18.129877 ],\n",
       "       [-19.649536 ,  17.741898 ],\n",
       "       [ -3.960147 ,  -4.0774217]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Compute the accuracy of the validation logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Data loaders \n",
    "\n",
    "Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "# create a dataset\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, a=3, b=8, n=10000):\n",
    "        x, y = gen_fake_data(n, a, b)\n",
    "        x = torch.from_numpy(x).unsqueeze(1)\n",
    "        y = torch.from_numpy(y)\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "fake_train_ds = RegressionDataset()\n",
    "fake_valid_ds = RegressionDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create a data loader. The data loader provides the following features:\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(fake_train_ds, batch_size=1000, shuffle=True)\n",
    "valid_dl = DataLoader(fake_valid_ds, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a batch of data\n",
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1]), torch.Size([1000]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def val_metric(model, valid_dl):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    y_hats = []\n",
    "    ys = []\n",
    "    for x, y in valid_dl:\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = model(x.float())\n",
    "        loss = F.mse_loss(y_hat, y.float())\n",
    "        y_hats.append(y_hat.detach().numpy())\n",
    "        ys.append(y.numpy())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    ys = np.concatenate(ys)\n",
    "    y_hats = np.concatenate(y_hats)\n",
    "    return np.mean(losses), r2_score(ys, y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.57502746582031, -83.13645542297598)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss, valid_r2 = val_metric(model2, valid_dl)\n",
    "valid_loss, valid_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_loop function\n",
    "def train_loop(model, train_dl, valid_dl, optimizer, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        for x, y in train_dl:\n",
    "            y = y.unsqueeze(1)\n",
    "            y_hat = model(x.float())\n",
    "            loss = F.mse_loss(y_hat, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(losses)\n",
    "        valid_loss, valid_auc = val_metric(model, valid_dl)\n",
    "        print(\"train loss %.3f valid loss %.3f auc roc %.3f\" % (train_loss, valid_loss, valid_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    ")\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 85.088 valid loss 70.502 auc roc -83.049\n",
      "train loss 72.447 valid loss 48.049 auc roc -56.282\n",
      "train loss 61.584 valid loss 31.017 auc roc -35.977\n",
      "train loss 52.472 valid loss 18.915 auc roc -21.550\n",
      "train loss 44.974 valid loss 10.891 auc roc -11.984\n",
      "train loss 38.888 valid loss 5.946 auc roc -6.088\n",
      "train loss 33.983 valid loss 3.140 auc roc -2.743\n",
      "train loss 30.036 valid loss 1.677 auc roc -1.000\n",
      "train loss 26.846 valid loss 0.983 auc roc -0.172\n",
      "train loss 24.245 valid loss 0.683 auc roc 0.186\n",
      "train loss 22.097 valid loss 0.562 auc roc 0.330\n",
      "train loss 20.300 valid loss 0.512 auc roc 0.390\n",
      "train loss 18.777 valid loss 0.485 auc roc 0.422\n",
      "train loss 17.470 valid loss 0.465 auc roc 0.446\n",
      "train loss 16.336 valid loss 0.445 auc roc 0.469\n",
      "train loss 15.342 valid loss 0.426 auc roc 0.492\n",
      "train loss 14.464 valid loss 0.407 auc roc 0.515\n",
      "train loss 13.682 valid loss 0.389 auc roc 0.537\n",
      "train loss 12.982 valid loss 0.371 auc roc 0.558\n",
      "train loss 12.351 valid loss 0.354 auc roc 0.579\n"
     ]
    }
   ],
   "source": [
    "train_loop(model2, train_dl, valid_dl, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.335 valid loss 0.319 auc roc 0.620\n",
      "train loss 0.318 valid loss 0.290 auc roc 0.654\n",
      "train loss 0.304 valid loss 0.264 auc roc 0.685\n",
      "train loss 0.291 valid loss 0.241 auc roc 0.712\n",
      "train loss 0.278 valid loss 0.221 auc roc 0.737\n",
      "train loss 0.267 valid loss 0.203 auc roc 0.758\n",
      "train loss 0.256 valid loss 0.187 auc roc 0.777\n",
      "train loss 0.246 valid loss 0.173 auc roc 0.794\n",
      "train loss 0.237 valid loss 0.160 auc roc 0.809\n",
      "train loss 0.228 valid loss 0.149 auc roc 0.822\n",
      "train loss 0.220 valid loss 0.140 auc roc 0.833\n",
      "train loss 0.213 valid loss 0.132 auc roc 0.842\n",
      "train loss 0.206 valid loss 0.125 auc roc 0.851\n",
      "train loss 0.200 valid loss 0.120 auc roc 0.858\n",
      "train loss 0.194 valid loss 0.115 auc roc 0.863\n",
      "train loss 0.189 valid loss 0.110 auc roc 0.868\n",
      "train loss 0.184 valid loss 0.107 auc roc 0.872\n",
      "train loss 0.180 valid loss 0.104 auc roc 0.876\n",
      "train loss 0.175 valid loss 0.102 auc roc 0.879\n",
      "train loss 0.171 valid loss 0.100 auc roc 0.881\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "train_loop(model2, train_dl, valid_dl, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Play with the training of the previous model to get the max auc possible. Can you use larger learning rates or more epochs? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "* https://pytorch.org/docs/stable/index.html\n",
    "* http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "116px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
